\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}

\newtheorem{defn}{Definition}
\newtheorem{theorem}{Theorem}

%opening
\title{UCLRL Lecture 2 Notes}
\author{}

\begin{document}

\maketitle

\section{Markov Decision Processes}

\subsection{Markov Reward Processes}

\begin{defn}
 A Mariv Process is a tuple $\left< \mathcal{S}, \mathcal{P} \right>$
\end{defn}


For a Markov state $s$ and successor state $s^\prime$, the state transition probability is defined by 
$$
\mathcal{P}_{s s^\prime} = \mathbb{P} \left[ S_{t+1} = s^\prime | S_t = s \right]
$$
and we can characterize the transition from all states by the transition matrix $\mathcal{P}$ where 
$$
\mathcal{P} =
\begin{bmatrix}
 \mathcal{P}_{11} && \ldots && \mathcal{P}_{1n} \\
 \vdots \\
 \mathcal{P}_{n1} && \ldots && \mathcal{P}_{nn} 
\end{bmatrix}
$$

\begin{defn}
 A Markov reward process is a Markov process but with a tuple $\left< \mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma \right>$ such that 
 \begin{itemize}
  \item $\mathcal{R}$ is a reward function such that $\mathcal{R}_s = \mathbb{E} \left[ R_{t+1} | S_t = s \right]$
  \item $\gamma$ is a discount factor with $\gamma \in \left[ 0, 1 \right]$
 \end{itemize}
\end{defn}

\begin{defn}
 The $\bm{return}$ $G_t$ is the total discounted reward from time-step $t$
 $$
 G_t = R_{t+1} + \gamma R_{t+2} + \ldots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
 $$
\end{defn}

\begin{defn}[State Value Function]
 The state value function $v(s)$ of an MRP is the expected return starting from state $s$
 $$
 v(s) = \mathbb{E} \left[ G_t | S_t = s \right]
 $$
\end{defn}

The value function be decomposed into
\begin{itemize}
 \item immediate reward $R_{t+1}$
 \item discounted value of successor state $\gamma v(S_{t+1}$
\end{itemize}

\begin{align*}
v(s) &= \mathbb{E} \left[ G_t | S_t = s \right] \\
&= \mathbb{E} \left[ R_{t+1} + \gamma G_{t+1} \right] \\
&= \mathbb{E} \left[ R_{t+1} + \gamma v(S_{t+1} \right] \\
\end{align*}

\begin{defn}[Bellman Equation for MRP]
 $$
 v(s) = \mathbb{E} \left[ R_{t+1} + \gamma v(S_{t+1} | S_t = s \right]
 $$
 which can be rewritten as
 $$
 v(s) = \mathcal{R}_s + \gamma \sum_{s^\prime \in \mathcal{S}} P_{s s^\prime} v(s^\prime)
 $$
 or in matrix notation
 $$
 v = \mathcal{R} + \gamma \mathcal{P} v
 $$
\end{defn}


Bellman equation can be solved direcly as
\begin{align*}
 v &= \mathcal{R} + \gamma \mathcal{P} v \\
   &= (I - \gamma \mathcal{P})^{-1} \mathcal{R}
\end{align*}


\subsection{Markov Decision Process}

\begin{defn}[Markov Decision Process]
A Markov Decision Process is a Markov Reward Process with a finite set of actions $\mathcal{A}$. Thus, the Markov Decision process is a tuple $\left< \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \right>$ such that
\begin{enumerate}
 \item $\mathcal{A}$ is a finite set of actions
 \item $\mathcal{P}_{s s^\prime}^a = \mathbb{P} \left[ S_{t+1} = s^\prime | S_t = s, A_t = a \right]$
 \item $\mathcal{R}$ is a reward function, $\mathcal{R}_s^a = \mathbb{E} \left[ R_{t+1} | S_t = s, A_t = a \right]$
\end{enumerate}
\end{defn}

\begin{defn}[Policy]
 A policy $\pi$ is a distribution over actions given states,
 $$
 \pi(a|s) = \mathbb{P} \left[ A_t = a | S_t = s \right]
 $$
\end{defn}

\begin{defn}[State Value Function]
 The state value function $v_\pi(s)$ of an MMDFP is the expected return starting from state $s$ and then following policy $\pi$
 $$
 v_\pi(s) = \mathbb{E}_\pi \left[ G_t | S_t = s \right]
 $$
\end{defn}

\begin{defn}[Action Value Function]
 The action value function $q_\pi(s,a)$ is the expected return staring from state $s$, taking action $a$, and then following policy $\pi$
 $$
 q_\pi(s,a) = \mathbb{E}_\pi \left[ G_t | S_t = s, A_t = a \right]
 $$
\end{defn}

\begin{defn}[Bellman Expectation Equation]
 Bellman expectation eqiation be expressed for the Markov reward process as
 $$
 v_\pi = \mathcal{R}^\pi + \gamma \mathcal{P}^\pi v_\pi
 $$
 which gives us the solution after solving for $v_\pi$, 
 $$
 v_\pi = \left(I - \gamma \mathcal{P}^\pi\right)^{-1} \mathcal{R}^\pi
 $$
\end{defn}

\begin{defn}
 The optimal state value function $v_o(s)$ is the maximum value function over all policies
 $$
 v_\star(s) = \max_\pi v_\pi(s)
 $$
\end{defn}

\begin{defn}
 The optimal action-value function $q_o(s,a)$ is the maximum action-value function over all policies
 $$
 q_\star(s,a) = \max_\pi q_\pi(s,a)
 $$
\end{defn}

\begin{defn}[Partial Ordering of Policies]
 $$
 \pi \geq \pi^\prime
 v_\pi(s) \geq v_\pi^\prime(s), \forall s
 $$
\end{defn}

\begin{theorem}
 For any Markov decision process
 \begin{enumerate}
  \item There exists an optimal policy $\pi_\star$ that is better than or equal to all other policies $\pi_\star \geq \pi, \forall \pi$
  \item All optimal policies achieve the optimal value function 
  $$
  v_{\pi_\star}(s) = v_\star(s)
  $$
  \item All optimal policies achive the optimal action-value function
  $$
  q_{\pi_\star}(s,a) = q_\star(s,a)
  $$
 \end{enumerate}

\end{theorem}

An optimal policy can be found by maximizing over $q_\star(s,a)$, 
$$
\pi_\star(a|s) = 
\begin{cases}
 1 & \mbox{if} a = {\arg\max}_{a \in \mathcal{A}} q_\star(s,a)\\
 0 & \mbox{otherwise} \\
\end{cases}
$$

\section{Bellman Equations}

For $v_\star$ we look at the action that gives us the most value,
$$
v_\star(s) = \max_a q_\star (s, a)
$$
and for $q_\star$, we have the immediate reward and the average of all the states and their values,
$$
q_\star(s,a) = \mathcal{R}_s^a + \gamma \sum_{s^\prime in \mathcal{S}} \mathcal{P}^a_{s s^\prime} v_\star(s^\prime)
$$
and combining them together,
$$
v_\star(s) = \max_a \left[ \mathcal{R}_s^a + \gamma \sum_{s^\prime \in \mathcal{S}} P^a_{s s^\prime} v_\star(s^\prime) \right]
$$
and similarly for $q_\star$, 
$$
q_\star(s,a) = \mathcal{R}_s^a + \gamma \sum_{s^\prime in \mathcal{S}} \mathcal{P}^a_{s s^\prime} \max_{a^\prime} q_\star(s^\prime, a^\prime)
$$

\end{document}