\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bm}

\newtheorem{defn}{Definition}

%opening
\title{UCLRL Lecture 2 Notes}
\author{}

\begin{document}

\maketitle

\section{Markov Decision Processes}

\subsection{Markov Reward Processes}

\begin{defn}
 A Mariv Process is a tuple $\left< \mathcal{S}, \mathcal{P} \right>$
\end{defn}


For a Markov state $s$ and successor state $s^\prime$, the state transition probability is defined by 
$$
\mathcal{P}_{s s^\prime} = \mathbb{P} \left[ S_{t+1} = s^\prime | S_t = s \right]
$$
and we can characterize the transition from all states by the transition matrix $\mathcal{P}$ where 
$$
\mathcal{P} =
\begin{bmatrix}
 \mathcal{P}_{11} && \ldots && \mathcal{P}_{1n} \\
 \vdots \\
 \mathcal{P}_{n1} && \ldots && \mathcal{P}_{nn} 
\end{bmatrix}
$$

\begin{defn}
 A Markov reward process is a Markov process but with a tuple $\left< \mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma \right>$ such that 
 \begin{itemize}
  \item $\mathcal{R}$ is a reward function such that $\mathcal{R}_s = \mathbb{E} \left[ R_{t+1} | S_t = s \right]$
  \item $\gamma$ is a discount factor with $\gamma \in \left[ 0, 1 \right]$
 \end{itemize}
\end{defn}

\begin{defn}
 The $\bm{return}$ $G_t$ is the total discounted reward from time-step $t$
 $$
 G_t = R_{t+1} + \gamma R_{t+2} + \ldots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
 $$
\end{defn}

\begin{defn}[State Value Function]
 The state value function $v(s)$ of an MRP is the expected return starting from state $s$
 $$
 v(s) = \mathbb{E} \left[ G_t | S_t = s \right]
 $$
\end{defn}

The value function be decomposed into
\begin{itemize}
 \item immediate reward $R_{t+1}$
 \item discounted value of successor state $\gamma v(S_{t+1}$
\end{itemize}

\begin{align*}
v(s) &= \mathbb{E} \left[ G_t | S_t = s \right] \\
&= \mathbb{E} \left[ R_{t+1} + \gamma G_{t+1} \right] \\
&= \mathbb{E} \left[ R_{t+1} + \gamma v(S_{t+1} \right] \\
\end{align*}

\begin{defn}[Bellman Equation for MRP]
 $$
 v(s) = \mathbb{E} \left[ R_{t+1} + \gamma v(S_{t+1} | S_t = s \right]
 $$
 which can be rewritten as
 $$
 v(s) = \mathcal{R}_s + \gamma \sum_{s^\prime \in \mathcal{S}} P_{s s^\prime} v(s^\prime)
 $$
 or in matrix notation
 $$
 v = \mathcal{R} + \gamma \mathcal{P} v
 $$
\end{defn}


Bellman equation can be solved direcly as
\begin{align*}
 v &= \mathcal{R} + \gamma \mathcal{P} v \\
   &= (I - \gamma \mathcal{P})^{-1} \mathcal{R}
\end{align*}


\end{document}